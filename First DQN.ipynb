{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing https://arxiv.org/abs/1312.5602 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try one of the simplest environment: Cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-08 16:24:11,495] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set all the seeds to 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "gym.spaces.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play one randome game with random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    env.render(close=True)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "print 'total reward:', total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_model(model):\n",
    "    # Requires Keras 1.0.7 since get_config has breaking changes.\n",
    "    config = {\n",
    "        'class_name': model.__class__.__name__,\n",
    "        'config': model.get_config(),\n",
    "    }\n",
    "    clone = model_from_config(config)\n",
    "    clone.set_weights(model.get_weights())\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add( Dense(output_dim = 20, input_shape = observation.shape ) )\n",
    "    model.add( Activation('sigmoid') )\n",
    "    model.add( Dense(output_dim = env.action_space.n ) )\n",
    "    model.add( Activation('linear') )\n",
    "\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.01) )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #Possible actions\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        \n",
    "        #NN\n",
    "        self.nn = make_model()\n",
    "        self.nn_target = make_model()\n",
    "        \n",
    "    def Q(self, observation, target = False):\n",
    "        model = self.nn\n",
    "        if target : model = self.nn_target\n",
    "        return model.predict(observation[None,:])[0]\n",
    "    \n",
    "    def Q_batch(self, observation, target = False):\n",
    "        model = self.nn\n",
    "        if target : model = self.nn_target\n",
    "        return model.predict_on_batch(observation)\n",
    "    \n",
    "    def sample_action(self, observation):\n",
    "        return np.argmax( self.Q(observation) )\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.nn_target.set_weights(self.nn.get_weights())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def play(agent, env, show = False, save = False): \n",
    "    if save : env.monitor.start('cartpole-experiment', force=True)\n",
    "\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_reward = 0.0\n",
    "    count = 0\n",
    "    while not done and count<2000: \n",
    "        q = agent.Q(observation)\n",
    "        action = np.argmax( q )\n",
    "        \n",
    "        if show: \n",
    "            env.render(\"human\")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        count += 1\n",
    "    \n",
    "    if save : env.monitor.close()\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the untrained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "print 'total reward:', play(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27 eps: 1.0 total reward: 27.0\n",
      "step: 39 eps: 0.9919 total reward: 12.0\n",
      "step: 61 eps: 0.9883 total reward: 22.0\n",
      "step: 79 eps: 0.9817 total reward: 18.0\n",
      "step: 91 eps: 0.9763 total reward: 12.0\n",
      "step: 100 eps: 0.9727 total reward: 9.0\n",
      "step: 113 eps: 0.97 total reward: 13.0\n",
      "step: 125 eps: 0.9661 total reward: 12.0\n",
      "step: 137 eps: 0.9625 total reward: 12.0\n",
      "step: 148 eps: 0.9589 total reward: 11.0\n",
      "step: 161 eps: 0.9556 total reward: 13.0\n",
      "step: 176 eps: 0.9517 total reward: 15.0\n",
      "step: 192 eps: 0.9472 total reward: 16.0\n",
      "step: 202 eps: 0.9424 total reward: 10.0\n",
      "step: 239 eps: 0.9394 total reward: 37.0\n",
      "step: 272 eps: 0.9283 total reward: 33.0\n",
      "step: 283 eps: 0.9184 total reward: 11.0\n",
      "step: 323 eps: 0.9151 total reward: 40.0\n",
      "step: 346 eps: 0.9031 total reward: 23.0\n",
      "step: 368 eps: 0.8962 total reward: 22.0\n",
      "step: 391 eps: 0.8896 total reward: 23.0\n",
      "step: 404 eps: 0.8827 total reward: 13.0\n",
      "step: 418 eps: 0.8788 total reward: 14.0\n",
      "step: 433 eps: 0.8746 total reward: 15.0\n",
      "step: 474 eps: 0.8701 total reward: 41.0\n",
      "step: 490 eps: 0.8578 total reward: 16.0\n",
      "step: 506 eps: 0.853 total reward: 16.0\n",
      "step: 539 eps: 0.8482 total reward: 33.0\n",
      "step: 563 eps: 0.8383 total reward: 24.0\n",
      "step: 581 eps: 0.8311 total reward: 18.0\n",
      "step: 590 eps: 0.8257 total reward: 9.0\n",
      "step: 601 eps: 0.823 total reward: 11.0\n",
      "step: 631 eps: 0.8197 total reward: 30.0\n",
      "step: 671 eps: 0.8107 total reward: 40.0\n",
      "step: 685 eps: 0.7987 total reward: 14.0\n",
      "step: 701 eps: 0.7945 total reward: 16.0\n",
      "step: 714 eps: 0.7897 total reward: 13.0\n",
      "step: 733 eps: 0.7858 total reward: 19.0\n",
      "step: 754 eps: 0.7801 total reward: 21.0\n",
      "step: 781 eps: 0.7738 total reward: 27.0\n",
      "step: 791 eps: 0.7657 total reward: 10.0\n",
      "step: 826 eps: 0.7627 total reward: 35.0\n",
      "step: 835 eps: 0.7522 total reward: 9.0\n",
      "step: 846 eps: 0.7495 total reward: 11.0\n",
      "step: 867 eps: 0.7462 total reward: 21.0\n",
      "step: 890 eps: 0.7399 total reward: 23.0\n",
      "step: 903 eps: 0.733 total reward: 13.0\n",
      "step: 915 eps: 0.7291 total reward: 12.0\n",
      "step: 929 eps: 0.7255 total reward: 14.0\n",
      "step: 944 eps: 0.7213 total reward: 15.0\n",
      "step: 963 eps: 0.7168 total reward: 19.0\n",
      "step: 990 eps: 0.7111 total reward: 27.0\n",
      "step: 1004 eps: 0.703 total reward: 14.0\n",
      "step: 1028 eps: 0.6988 total reward: 24.0\n",
      "step: 1049 eps: 0.6916 total reward: 21.0\n",
      "step: 1066 eps: 0.6853 total reward: 17.0\n",
      "step: 1076 eps: 0.6802 total reward: 10.0\n",
      "step: 1094 eps: 0.6772 total reward: 18.0\n",
      "step: 1114 eps: 0.6718 total reward: 20.0\n",
      "step: 1142 eps: 0.6658 total reward: 28.0\n",
      "step: 1154 eps: 0.6574 total reward: 12.0\n",
      "step: 1169 eps: 0.6538 total reward: 15.0\n",
      "step: 1187 eps: 0.6493 total reward: 18.0\n",
      "step: 1205 eps: 0.6439 total reward: 18.0\n",
      "step: 1222 eps: 0.6385 total reward: 17.0\n",
      "step: 1254 eps: 0.6334 total reward: 32.0\n",
      "step: 1276 eps: 0.6238 total reward: 22.0\n",
      "step: 1299 eps: 0.6172 total reward: 23.0\n",
      "step: 1339 eps: 0.6103 total reward: 40.0\n",
      "step: 1368 eps: 0.5983 total reward: 29.0\n",
      "step: 1405 eps: 0.5896 total reward: 37.0\n",
      "step: 1416 eps: 0.5785 total reward: 11.0\n",
      "step: 1451 eps: 0.5752 total reward: 35.0\n",
      "step: 1549 eps: 0.5647 total reward: 98.0\n",
      "step: 1622 eps: 0.5353 total reward: 73.0\n",
      "step: 1675 eps: 0.5134 total reward: 53.0\n",
      "step: 1703 eps: 0.4975 total reward: 28.0\n",
      "step: 1718 eps: 0.4891 total reward: 15.0\n",
      "step: 1797 eps: 0.4846 total reward: 79.0\n",
      "step: 1839 eps: 0.4609 total reward: 42.0\n",
      "step: 1912 eps: 0.4483 total reward: 73.0\n",
      "step: 1946 eps: 0.4264 total reward: 34.0\n",
      "step: 1995 eps: 0.4162 total reward: 49.0\n",
      "step: 2036 eps: 0.4015 total reward: 41.0\n",
      "step: 2067 eps: 0.3892 total reward: 31.0\n",
      "step: 2097 eps: 0.3799 total reward: 30.0\n",
      "step: 2130 eps: 0.3709 total reward: 33.0\n",
      "step: 2170 eps: 0.361 total reward: 40.0\n",
      "step: 2224 eps: 0.349 total reward: 54.0\n",
      "step: 2249 eps: 0.3328 total reward: 25.0\n",
      "step: 2285 eps: 0.3253 total reward: 36.0\n",
      "step: 2314 eps: 0.3145 total reward: 29.0\n",
      "step: 2406 eps: 0.3058 total reward: 92.0\n",
      "step: 2497 eps: 0.2782 total reward: 91.0\n",
      "step: 2550 eps: 0.2509 total reward: 53.0\n",
      "step: 2683 eps: 0.235 total reward: 133.0\n",
      "step: 2718 eps: 0.1951 total reward: 35.0\n",
      "step: 2779 eps: 0.1846 total reward: 61.0\n",
      "step: 2850 eps: 0.1663 total reward: 71.0\n",
      "step: 2885 eps: 0.145 total reward: 35.0\n",
      "step: 2945 eps: 0.1345 total reward: 60.0\n",
      "step: 3028 eps: 0.1165 total reward: 83.0\n",
      "step: 3088 eps: 0.1 total reward: 60.0\n",
      "step: 3132 eps: 0.1 total reward: 44.0\n",
      "step: 3197 eps: 0.1 total reward: 65.0\n",
      "step: 3235 eps: 0.1 total reward: 38.0\n",
      "step: 3328 eps: 0.1 total reward: 93.0\n",
      "step: 3405 eps: 0.1 total reward: 77.0\n",
      "step: 3434 eps: 0.1 total reward: 29.0\n",
      "step: 3507 eps: 0.1 total reward: 73.0\n",
      "step: 3579 eps: 0.1 total reward: 72.0\n",
      "step: 3628 eps: 0.1 total reward: 49.0\n",
      "step: 3711 eps: 0.1 total reward: 83.0\n",
      "step: 3803 eps: 0.1 total reward: 92.0\n",
      "step: 3867 eps: 0.1 total reward: 64.0\n",
      "step: 3954 eps: 0.1 total reward: 87.0\n",
      "step: 4034 eps: 0.1 total reward: 80.0\n",
      "step: 4108 eps: 0.1 total reward: 74.0\n",
      "step: 4189 eps: 0.1 total reward: 81.0\n",
      "step: 4272 eps: 0.1 total reward: 83.0\n",
      "step: 4352 eps: 0.1 total reward: 80.0\n",
      "step: 4440 eps: 0.1 total reward: 88.0\n",
      "step: 4534 eps: 0.1 total reward: 94.0\n",
      "step: 4589 eps: 0.1 total reward: 55.0\n",
      "step: 4644 eps: 0.1 total reward: 55.0\n",
      "step: 4751 eps: 0.1 total reward: 107.0\n",
      "step: 4865 eps: 0.1 total reward: 114.0\n",
      "step: 4990 eps: 0.1 total reward: 125.0\n",
      "step: 5067 eps: 0.1 total reward: 77.0\n",
      "step: 5166 eps: 0.1 total reward: 99.0\n",
      "step: 5245 eps: 0.1 total reward: 79.0\n",
      "step: 5276 eps: 0.1 total reward: 31.0\n",
      "step: 5359 eps: 0.1 total reward: 83.0\n",
      "step: 5454 eps: 0.1 total reward: 95.0\n",
      "step: 5515 eps: 0.1 total reward: 61.0\n",
      "step: 5600 eps: 0.1 total reward: 85.0\n",
      "step: 5745 eps: 0.1 total reward: 145.0\n",
      "step: 5868 eps: 0.1 total reward: 123.0\n",
      "step: 6031 eps: 0.1 total reward: 163.0\n",
      "step: 6167 eps: 0.1 total reward: 136.0\n",
      "step: 6490 eps: 0.1 total reward: 323.0\n",
      "step: 6697 eps: 0.1 total reward: 207.0\n",
      "step: 7064 eps: 0.1 total reward: 367.0\n",
      "step: 7698 eps: 0.1 total reward: 634.0\n",
      "step: 8481 eps: 0.1 total reward: 783.0\n",
      "step: 8727 eps: 0.1 total reward: 246.0\n",
      "step: 10000 eps: 0.1 total reward: 1273.0\n",
      "CPU times: user 21.8 s, sys: 160 ms, total: 22 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "gym.spaces.seed(42)\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "N_steps = 10000\n",
    "N = 50000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "C = 20\n",
    "\n",
    "D = deque(maxlen=N)\n",
    "\n",
    "step = 0\n",
    "while step<N_steps:\n",
    "    eps = max( 0.1, 1.0 - 3.0*step/float(N_steps) )\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done and step<N_steps: \n",
    "        if np.random.binomial(1, p=eps) == 1:\n",
    "            action = np.random.choice( env.action_space.n )\n",
    "        else:\n",
    "            action = agent.sample_action(obs)\n",
    "            \n",
    "        obs_p, reward, done, info = env.step( action )\n",
    "        total_reward += reward\n",
    "\n",
    "        D.append( {'obs':obs, 'action':action, 'reward':reward, 'obs_p':obs_p, 'done':done} )\n",
    "            \n",
    "        obs = obs_p\n",
    "\n",
    "        batch = np.random.choice( D , size=min(batch_size, len(D)), replace=False )\n",
    "        \n",
    "        batch_obs = []\n",
    "        batch_obs_p = []\n",
    "        for b in batch:\n",
    "            batch_obs.append(b['obs'])\n",
    "            batch_obs_p.append(b['obs_p'])\n",
    "        \n",
    "        batch_obs = np.array(batch_obs)\n",
    "        batch_obs_p = np.array(batch_obs_p)\n",
    "        \n",
    "        Q_pred = np.max( agent.Q_batch(batch_obs_p, target=True), axis = 1 )\n",
    "        \n",
    "        #So cheap, same trick as in keras-rl\n",
    "        y = agent.Q_batch(batch_obs)\n",
    "        for i, b in enumerate(batch):            \n",
    "            y[i, b['action'] ] = b['reward']\n",
    "            if not b['done'] :\n",
    "                y[i, b['action'] ] += gamma*Q_pred[i]\n",
    "            \n",
    "        agent.nn.train_on_batch( batch_obs , y )\n",
    "        \n",
    "        if step%C == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    print 'step:', step, 'eps:', eps, 'total reward:', total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 759.0\n"
     ]
    }
   ],
   "source": [
    "print 'total reward:', play(agent, env, save =  False, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
