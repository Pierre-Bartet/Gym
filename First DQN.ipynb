{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing https://arxiv.org/abs/1312.5602 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try one of the simplest environment: Cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-08 13:13:27,366] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set all the seeds to 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "gym.spaces.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play one randome game with random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    env.render(close=True)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "print 'total reward:', total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #Possible actions\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        \n",
    "        #NN\n",
    "        self.nn = Sequential()\n",
    "\n",
    "        self.nn.add( Dense(output_dim = 20, input_shape = observation.shape ) )\n",
    "        self.nn.add( Activation('sigmoid') )\n",
    "        self.nn.add( Dense(output_dim = env.action_space.n ) )\n",
    "        self.nn.add( Activation('linear') )\n",
    "\n",
    "        self.nn.compile(loss='mse', optimizer=SGD(lr=0.01) )\n",
    "        \n",
    "    def Q(self, observation):\n",
    "        return self.nn.predict(observation[None,:])[0]\n",
    "    \n",
    "    def Q_batch(self, observation):\n",
    "        return self.nn.predict_on_batch(observation)\n",
    "    \n",
    "    def sample_action(self, observation):\n",
    "        return np.argmax( self.Q(observation) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def play(agent, env, show = False, save = False): \n",
    "    if save : env.monitor.start('cartpole-experiment', force=True)\n",
    "\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_reward = 0.0\n",
    "    count = 0\n",
    "    while not done and count<2000: \n",
    "        q = agent.Q(observation)\n",
    "        action = np.argmax( q )\n",
    "        \n",
    "        if show: \n",
    "            env.render(\"human\")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        count += 1\n",
    "    \n",
    "    if save : env.monitor.close()\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the untrained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "print 'total reward:', play(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27 eps: 1.0 total reward: 27.0\n",
      "step: 39 eps: 0.973 total reward: 12.0\n",
      "step: 61 eps: 0.961 total reward: 22.0\n",
      "step: 78 eps: 0.939 total reward: 17.0\n",
      "step: 95 eps: 0.922 total reward: 17.0\n",
      "step: 110 eps: 0.905 total reward: 15.0\n",
      "step: 120 eps: 0.89 total reward: 10.0\n",
      "step: 146 eps: 0.88 total reward: 26.0\n",
      "step: 162 eps: 0.854 total reward: 16.0\n",
      "step: 184 eps: 0.838 total reward: 22.0\n",
      "step: 199 eps: 0.816 total reward: 15.0\n",
      "step: 216 eps: 0.801 total reward: 17.0\n",
      "step: 240 eps: 0.784 total reward: 24.0\n",
      "step: 266 eps: 0.76 total reward: 26.0\n",
      "step: 280 eps: 0.734 total reward: 14.0\n",
      "step: 366 eps: 0.72 total reward: 86.0\n",
      "step: 391 eps: 0.634 total reward: 25.0\n",
      "step: 435 eps: 0.609 total reward: 44.0\n",
      "step: 522 eps: 0.565 total reward: 87.0\n",
      "step: 611 eps: 0.478 total reward: 89.0\n",
      "step: 738 eps: 0.389 total reward: 127.0\n",
      "step: 854 eps: 0.262 total reward: 116.0\n",
      "step: 1008 eps: 0.146 total reward: 154.0\n",
      "step: 1132 eps: 0.1 total reward: 124.0\n",
      "step: 1164 eps: 0.1 total reward: 32.0\n",
      "step: 1190 eps: 0.1 total reward: 26.0\n",
      "step: 1250 eps: 0.1 total reward: 60.0\n",
      "step: 1342 eps: 0.1 total reward: 92.0\n",
      "step: 1514 eps: 0.1 total reward: 172.0\n",
      "step: 1729 eps: 0.1 total reward: 215.0\n",
      "step: 1826 eps: 0.1 total reward: 97.0\n",
      "step: 1842 eps: 0.1 total reward: 16.0\n",
      "step: 1907 eps: 0.1 total reward: 65.0\n",
      "step: 1974 eps: 0.1 total reward: 67.0\n",
      "step: 2337 eps: 0.1 total reward: 363.0\n",
      "step: 2419 eps: 0.1 total reward: 82.0\n",
      "step: 2581 eps: 0.1 total reward: 162.0\n",
      "step: 2676 eps: 0.1 total reward: 95.0\n",
      "step: 2933 eps: 0.1 total reward: 257.0\n",
      "step: 3000 eps: 0.1 total reward: 67.0\n",
      "CPU times: user 5.12 s, sys: 58.6 ms, total: 5.18 s\n",
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "gym.spaces.seed(42)\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "N_steps = 10000\n",
    "N = 50000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "D = deque(maxlen=N)\n",
    "\n",
    "step = 0\n",
    "while step<N_steps:\n",
    "    eps = max( 0.1, 1.0 - 3.0*step/float(N_steps) )\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done and step<N_steps: \n",
    "        if np.random.binomial(1, p=eps) == 1:\n",
    "            action = np.random.choice( env.action_space.n )\n",
    "        else:\n",
    "            action = agent.sample_action(obs)\n",
    "            \n",
    "        obs_p, reward, done, info = env.step( action )\n",
    "        total_reward += reward\n",
    "\n",
    "        D.append( {'obs':obs, 'action':action, 'reward':reward, 'obs_p':obs_p, 'done':done} )\n",
    "            \n",
    "        obs = obs_p\n",
    "\n",
    "        batch = np.random.choice( D , size=min(batch_size, len(D)), replace=False )\n",
    "        \n",
    "        batch_obs = []\n",
    "        batch_obs_p = []\n",
    "        for b in batch:\n",
    "            batch_obs.append(b['obs'])\n",
    "            batch_obs_p.append(b['obs_p'])\n",
    "        \n",
    "        batch_obs = np.array(batch_obs)\n",
    "        batch_obs_p = np.array(batch_obs_p)\n",
    "        \n",
    "        Q_pred = np.max( agent.Q_batch(batch_obs_p), axis = 1 )\n",
    "        \n",
    "        #So cheap, same trick as in keras-rl\n",
    "        y = agent.Q_batch(batch_obs)\n",
    "        for i, b in enumerate(batch):            \n",
    "            y[i, b['action'] ] = b['reward']\n",
    "            if not b['done'] :\n",
    "                y[i, b['action'] ] += gamma*Q_pred[i]\n",
    "            \n",
    "        agent.nn.train_on_batch( batch_obs , y )\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    print 'step:', step, 'eps:', eps, 'total reward:', total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 240.0\n"
     ]
    }
   ],
   "source": [
    "print 'total reward:', play(agent, env, save =  True, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
