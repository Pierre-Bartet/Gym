{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing https://arxiv.org/abs/1312.5602 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try one of the simplest environment: Cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-08 11:23:57,896] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set all the seeds to 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "gym.spaces.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play one randome game with random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    env.render(close=True)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "print 'total reward:', total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #Possible actions\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        \n",
    "        #NN\n",
    "        self.nn = Sequential()\n",
    "\n",
    "        self.nn.add( Dense(output_dim = 20, input_shape = observation.shape ) )\n",
    "        self.nn.add( Activation('sigmoid') )\n",
    "        self.nn.add( Dense(output_dim = env.action_space.n ) )\n",
    "        self.nn.add( Activation('linear') )\n",
    "\n",
    "        self.nn.compile(loss='mse', optimizer=SGD(lr=0.1) )\n",
    "        \n",
    "    def Q(self, observation):\n",
    "        return self.nn.predict(observation[None,:])[0]\n",
    "    \n",
    "    def Q_batch(self, observation):\n",
    "        return self.nn.predict_on_batch(observation)\n",
    "    \n",
    "    def sample_action(self, observation):\n",
    "        return np.argmax( self.Q(observation) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def play(agent, env, first_observation, show = False): \n",
    "    e = copy.deepcopy(env)\n",
    "    \n",
    "    done = False\n",
    "    observation = first_observation\n",
    "    total_reward = 0.0\n",
    "    count = 0\n",
    "    while not done and count<2000: \n",
    "        q = agent.Q(observation)\n",
    "        action = np.argmax( q )\n",
    "        \n",
    "        if show : \n",
    "            e.render()\n",
    "        \n",
    "        observation, reward, done, info = e.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        count += 1\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the untrained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q for the initial state: [ 0.40602553 -0.40782896]\n",
      "total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "first_observation = env.reset()\n",
    "print 'Q for the initial state:', agent.Q( first_observation )\n",
    "print 'total reward:', play(agent, env, first_observation, show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15 eps: 1.0 total reward: 15.0\n",
      "step: 31 eps: 0.975 total reward: 16.0\n",
      "step: 69 eps: 0.948333333333 total reward: 38.0\n",
      "step: 81 eps: 0.885 total reward: 12.0\n",
      "step: 92 eps: 0.865 total reward: 11.0\n",
      "step: 106 eps: 0.846666666667 total reward: 14.0\n",
      "step: 115 eps: 0.823333333333 total reward: 9.0\n",
      "step: 196 eps: 0.808333333333 total reward: 81.0\n",
      "step: 213 eps: 0.673333333333 total reward: 17.0\n",
      "step: 248 eps: 0.645 total reward: 35.0\n",
      "step: 279 eps: 0.586666666667 total reward: 31.0\n",
      "step: 324 eps: 0.535 total reward: 45.0\n",
      "step: 356 eps: 0.46 total reward: 32.0\n",
      "step: 434 eps: 0.406666666667 total reward: 78.0\n",
      "step: 533 eps: 0.276666666667 total reward: 99.0\n",
      "step: 778 eps: 0.111666666667 total reward: 245.0\n",
      "step: 1104 eps: 0.1 total reward: 326.0\n",
      "step: 1275 eps: 0.1 total reward: 171.0\n",
      "step: 1603 eps: 0.1 total reward: 328.0\n",
      "step: 1905 eps: 0.1 total reward: 302.0\n",
      "step: 2226 eps: 0.1 total reward: 321.0\n",
      "step: 2248 eps: 0.1 total reward: 22.0\n",
      "step: 2351 eps: 0.1 total reward: 103.0\n",
      "step: 2513 eps: 0.1 total reward: 162.0\n",
      "step: 2773 eps: 0.1 total reward: 260.0\n",
      "step: 3000 eps: 0.1 total reward: 227.0\n",
      "CPU times: user 3.71 s, sys: 35.8 ms, total: 3.75 s\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "N_steps = 3000\n",
    "N = 50000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "D = deque(maxlen=N)\n",
    "\n",
    "step = 0\n",
    "while step<N_steps:\n",
    "    eps = max( 0.1, 1.0 - 5.0*step/float(N_steps) )\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done and step<N_steps: \n",
    "        if np.random.binomial(1, p=eps) == 1:\n",
    "            action = np.random.choice( env.action_space.n )\n",
    "        else:\n",
    "            action = agent.sample_action(obs)\n",
    "            \n",
    "        obs_p, reward, done, info = env.step( action )\n",
    "        total_reward += reward\n",
    "\n",
    "        D.append( {'obs':obs, 'action':action, 'reward':reward, 'obs_p':obs_p, 'done':done} )\n",
    "            \n",
    "        obs = obs_p\n",
    "\n",
    "        batch = np.random.choice( D , size=min(batch_size, len(D)), replace=False )\n",
    "        \n",
    "        batch_obs = []\n",
    "        batch_obs_p = []\n",
    "        for b in batch:\n",
    "            batch_obs.append(b['obs'])\n",
    "            batch_obs_p.append(b['obs_p'])\n",
    "        \n",
    "        batch_obs = np.array(batch_obs)\n",
    "        batch_obs_p = np.array(batch_obs_p)\n",
    "        \n",
    "        Q_pred = np.max( agent.Q_batch(batch_obs_p), axis = 1 )\n",
    "        \n",
    "        #So cheap, same trick as in keras-rl\n",
    "        y = agent.Q_batch(batch_obs)\n",
    "        for i, b in enumerate(batch):            \n",
    "            y[i, b['action'] ] = b['reward']\n",
    "            if not b['done'] :\n",
    "                y[i, b['action'] ] += gamma*Q_pred[i]\n",
    "            \n",
    "        agent.nn.train_on_batch( batch_obs , y )\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    print 'step:', step, 'eps:', eps, 'total reward:', total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q for the initial state: [ 214.53590393  218.30377197]\n",
      "total reward: 2000.0\n"
     ]
    }
   ],
   "source": [
    "first_observation = env.reset()\n",
    "print 'Q for the initial state:', agent.Q( first_observation )\n",
    "print 'total reward:', play(agent, env, first_observation, show = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
